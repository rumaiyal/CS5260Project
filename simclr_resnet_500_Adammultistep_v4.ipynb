{"cells":[{"cell_type":"markdown","metadata":{"id":"HptO0CSwNjie"},"source":["# Contrastive Learning on CIFAR10 using Resnet50 Backbone"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIWaDsVYG4OH","outputId":"59a86222-a8f5-44d4-fedf-c00b7d53f264","executionInfo":{"status":"ok","timestamp":1652836266171,"user_tz":-480,"elapsed":27448,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"vXC8nEmxOMN6"},"source":["First, we import the dataset and define transformation operations on it. We apply random transformation on images (crop + flip + colorjitter + grayscale)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"c61bUj3dVpYD","executionInfo":{"status":"ok","timestamp":1652836268845,"user_tz":-480,"elapsed":2678,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","import torch\n","\n","class CIFAR10Pair(CIFAR10):\n","    \"\"\"CIFAR10 Dataset.\n","    \"\"\"\n","\n","    def __getitem__(self, index):\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            pos_1 = self.transform(img)\n","            pos_2 = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return pos_1, pos_2, target\n","\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(32),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n","    transforms.RandomGrayscale(p=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"]},{"cell_type":"markdown","metadata":{"id":"ct01fnfSNHuT"},"source":["We use commonly used ResNet-50 as ConvNet encoders for simplicity in the original paper. The task 1 is to set encoder and projection head. The parameters are adapted from the original paper."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZbjYxzrgG6rO","executionInfo":{"status":"ok","timestamp":1652836268845,"user_tz":-480,"elapsed":15,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models.resnet import resnet50\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, feature_dim=128):\n","        super(Model, self).__init__()\n","\n","        self.f = []\n","        for name, module in resnet50().named_children():\n","            if name == 'conv1':\n","                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n","                self.f.append(module)\n","        # ----------------------------------------------------------------------\n","        # START OF YOUR CODE\n","        # ----------------------------------------------------------------------\n","        # Task 1\n","        # set a neural network base encoder self.f\n","        # hint: nn.Sequential\n","        # Reference : https://github.com/leftthomas/SimCLR/blob/master/model.py\n","        self.f = nn.Sequential(*self.f)\n","\n","\n","        # set a small neural network projection head\n","        # Dense-> Relu-> Dense (2-layer MLP to project the representation to a 128-dimensional latent space and \n","        # the representation is 2048-dimensional here)\n","        # Reference : https://github.com/leftthomas/SimCLR/blob/master/model.py\n","        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512),\n","                               nn.ReLU(inplace=True), nn.Linear(512, feature_dim, bias=True))\n","     \n","\n","        \n","        # ----------------------------------------------------------------------\n","        # END OF YOUR CODE\n","        # ----------------------------------------------------------------------\n","    def forward(self, x):\n","        x = self.f(x)\n","        feature = torch.flatten(x, start_dim=1)\n","        out = self.g(feature)\n","        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n"]},{"cell_type":"markdown","metadata":{"id":"PPM5hsulQ74i"},"source":["We train encoder network and projection head to maximize agreement using a contrastive loss. The default epoch is 1 for time efficiency while it could takes about 10 minutes to run for one epoch in google colab. The task 2 is to calculate the contrastive loss.\n","To evaluate the influence of temperature value for contrastive loss, we run this training process 3 times with different temperature value (0.1,0.5 and 1.0)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7FrLDw2HAWN","outputId":"1049ad96-dda7-40c6-ece0-e421fc2c3fc7","executionInfo":{"status":"ok","timestamp":1652836272325,"user_tz":-480,"elapsed":3492,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thop\n","  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (4.2.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.0.31.post2005241907\n"]}],"source":["import argparse\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.optim as optim\n","!pip install thop\n","from thop import profile, clever_format\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","\n","import math\n","\n","def contrastive_loss(out_1, out_2, temperature):\n","\n","    # ------------------------------------------------------------------\n","    # START OF YOUR CODE\n","    # ------------------------------------------------------------------\n","    # Task2: implement contrastive loss function and return loss variable\n","    # hint: loss formula could refer to the slides\n","    # input: out_1, out_2，temperature\n","    # output: loss variable\n","\n","    out = torch.cat([out_1, out_2], dim=0)\n","    # [2*B, 2*B]\n","    sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n","    mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n","    # [2*B, 2*B-1]\n","    sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n","\n","    # compute loss\n","    pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n","    # [2*B]\n","    pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n","    loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean() \n","\n","    # ------------------------------------------------------------------\n","    # END OF YOUR CODE\n","    # ------------------------------------------------------------------\n","\n","    return loss\n","\n","# train for one epoch to learn unique features\n","def train(net, data_loader, train_optimizer, temperature):\n","    net.train()\n","    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n","    for pos_1, pos_2, target in train_bar:\n","        pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)\n","        feature_1, out_1 = net(pos_1)\n","        feature_2, out_2 = net(pos_2)\n","\n","        loss = contrastive_loss(out_1, out_2, temperature)\n","\n","        train_optimizer.zero_grad()\n","        loss.backward()\n","        train_optimizer.step()\n","        #train_scheduler.step()\n","\n","        total_num += batch_size\n","        total_loss += loss.item() * batch_size\n","        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n","\n","    return total_loss / total_num\n","\n","\n","# test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n","def test(net, memory_data_loader, test_data_loader, temperature):\n","    net.eval()\n","    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n","    with torch.no_grad():\n","        # generate feature bank\n","        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n","            feature, out = net(data.cuda(non_blocking=True))\n","            feature_bank.append(feature)\n","        # [D, N]\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        # [N]\n","        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n","        # loop test data to predict the label by weighted knn search\n","        test_bar = tqdm(test_data_loader)\n","        for data, _, target in test_bar:\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            feature, out = net(data)\n","\n","            total_num += data.size(0)\n","            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n","            sim_matrix = torch.mm(feature, feature_bank)\n","            # [B, K]\n","            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n","            # [B, K]\n","            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n","            sim_weight = (sim_weight / temperature).exp()\n","\n","            # counts for each class\n","            one_hot_label = torch.zeros(data.size(0) * k, c, device=sim_labels.device)\n","            # [B*K, C]\n","            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n","            # weighted score ---> [B, C]\n","            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n","\n","            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n","                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n","\n","    return total_top1 / total_num * 100, total_top5 / total_num * 100"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5nhS6egtxDk8","executionInfo":{"status":"ok","timestamp":1652836272326,"user_tz":-480,"elapsed":16,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":888,"referenced_widgets":["62ee5a27c50a43cf883d6306a0f85e00","c8d60e8cdf1d4bafbebd0c1efb645f6c","15299c55aa9f4ccfb2667b99318a786f","c413769aa99f4593b5f4975b6ff7e83a","5a7f2822e9484e8b9ceb1261300a294f","8df16e45245f425c85346691eec10bb3","42b5e4a93be448bbb10023714d6af295","65460bce30fa4e4aa1e278f157895d6c","66227592da44435ab0256fb9abd92a2a","037f87f320654c569df992dcb481a8db","f804fb57c7824e6d84078aeefea687db"]},"id":"BCIhOUGGxT93","outputId":"e408c8cd-db30-4168-edb0-0dfd1eeb92f6","scrolled":false,"executionInfo":{"status":"error","timestamp":1652836464459,"user_tz":-480,"elapsed":192146,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62ee5a27c50a43cf883d6306a0f85e00"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n","[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.Model'>. Treat it as zero Macs and zero Params.\u001b[00m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/thop/vision/basic_hooks.py:92: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  kernel = torch.DoubleTensor([*(x[0].shape[2:])]) // torch.DoubleTensor(list((m.output_size,))).squeeze()\n"]},{"output_type":"stream","name":"stdout","text":["# Model Params: 24.62M FLOPs: 1.31G\n","[5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330]\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: [1/500] Loss: 4.8777: 100%|██████████| 390/390 [01:57<00:00,  3.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.001875\n","tensor(4.8777)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:21<00:00, 18.01it/s]\n","Test Epoch: [1/500] Acc@1:40.78% Acc@5:89.99%: 100%|██████████| 79/79 [00:06<00:00, 12.49it/s]\n","Train Epoch: [2/500] Loss: 4.3210:   4%|▍         | 15/390 [00:10<04:34,  1.37it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e6559a452f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mtrain_loss_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-4b8f40a5278a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, train_optimizer, temperature)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m#train_scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train SimCLR\n","import numpy as np\n","   \n","# Feature dim for latent vector, Temperature used in softmax, Top k most similar images used to predict the label\n","feature_dim, temp, k = 128, [0.1], 200\n","# Number of images in each mini-batch, Number of sweeps over the dataset to train\n","batch_size=128\n","max_lrvalue=0.001875 #Rule of Thumb is peaklr (from lambdalr test)*3/8\n","min_lrvalue=0.0001\n","epochs=500\n","numsteps=np.max([epochs*0.67/5,10]).astype(int) \n","temp0 = 0.1 #contrastive loss temperature setting\n","schedulertype='multisteplr' #'multisteplr' for actual training\n","if schedulertype=='multisteplr':\n","    str1=str(min_lrvalue)\n","    str2=str(max_lrvalue)\n","    str3=str(numsteps)\n","    #epochs=3\n","else:\n","    print('choose valid option for scheduler')\n","smoothfactor=0.95 #Smooth Factor for smoothing contrastive loss    \n","IterationStr='It1'\n","loadmodel=0 #loadmodel=0 From scratch or loadmodel=1 Continue from presaved model \n","pathtosavemodel='/content/gdrive/MyDrive/CS5260Project/results/Simclr_resnet50_Adamv4'+schedulertype+'_minlr_'+str1+'_maxlr_'+str2+'_numsteps_'+str3+'/'+IterationStr+'/model/'\n","pathtosavecsv='/content/gdrive/MyDrive/CS5260Project/results/Simclr_resnet50_Adamv4'+schedulertype+'_minlr_'+str1+'_maxlr_'+str2+'_numsteps_'+str3+'/'+IterationStr+'/csv/'\n","\n","save_name_pre = '{}_{}_{}_{}_{}'.format(feature_dim, temp0, k, batch_size, epochs)\n","csvfilename=pathtosavecsv+'{}_statistics.csv'.format(save_name_pre)\n","modelfilename=pathtosavemodel+'{}_model.pth'.format(save_name_pre)\n","\n","# data prepare\n","train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True,\n","                          drop_last=True)\n","memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n","memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n","test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n","\n","import torch\n","n=0\n","torch.cuda.is_available()\n","#torch.cuda.set_device(n)\n","\n","# model setup and optimizer config\n","model = Model(feature_dim).cuda()\n","\n","flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n","flops, params = clever_format([flops, params])\n","print('# Model Params: {} FLOPs: {}'.format(params, flops))\n","\n","optimizer = optim.Adam(model.parameters(), weight_decay=1e-6, lr=max_lrvalue)   \n","if schedulertype=='multisteplr':\n","   gammaval=2**(math.log2(min_lrvalue/max_lrvalue)/numsteps)\n","   milestones_list=np.rint(np.arange(0,numsteps)/numsteps*epochs*0.67)[1:].astype(int).tolist() \n","   print(milestones_list)\n","   scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=milestones_list,gamma=gammaval)\n","else:\n","   print('choose valid option for scheduler')\n","\n","if loadmodel==1:\n","   checkpoint=torch.load(modelfilename)\n","   model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n","   model.to(device)\n","   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","   scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","   startepoch=checkpoint['epoch']+1\n","   best_acc=checkpoint['best_acc']\n","\n","   print(startepoch)\n","else:     \n","   startepoch=1\n","   best_acc=0\n","   \n","\n","c = len(memory_data.classes)\n","\n","if not os.path.exists(pathtosavemodel):\n","   os.makedirs(pathtosavemodel)\n","if not os.path.exists(pathtosavecsv):\n","   os.makedirs(pathtosavecsv)\n","\n","train_loss_epoch=torch.zeros(epochs)\n","smooth_loss_epoch=torch.zeros(epochs)\n","test_acc_1_epoch=torch.zeros(epochs)\n","test_acc_5_epoch=torch.zeros(epochs)\n","lr_epoch=torch.zeros(epochs)\n","\n","if loadmodel==1:\n","\n","   df=pd.read_csv(csvfilename)\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,1]).apply(np.array)\n","   train_loss_epoch[0:temp.size]=torch.tensor(temp)\n","   train_loss_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,2]).apply(np.array)\n","   test_acc_1_epoch[0:temp.size]=torch.tensor(temp)\n","   test_acc_1_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,3]).apply(np.array)\n","   test_acc_5_epoch[0:temp.size]=torch.tensor(temp)\n","   test_acc_5_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,4]).apply(np.array)\n","   smooth_loss_epoch[0:temp.size]=torch.tensor(temp)\n","   smooth_loss_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,5]).apply(np.array)\n","   lr_epoch[0:temp.size]=torch.tensor(temp)\n","   lr_list=temp.tolist()\n","   results = {'train_loss': train_loss_list, 'test_acc@1': test_acc_1_list, 'test_acc@5': test_acc_5_list, 'smooth_loss': smooth_loss_list, 'lr_epoch': lr_list}\n","\n","else:\n","   results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': [], 'smooth_loss': [], 'lr_epoch': []}\n","\n","\n","for epoch in range(startepoch, epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, temp0)\n","    scheduler.step()\n","    train_loss_epoch[epoch-1]=train_loss\n","    if epoch>1:\n","       smooth_loss=float(train_loss_epoch[epoch-1]*smoothfactor+smooth_loss_epoch[epoch-2]*(1.0-smoothfactor))\n","    else:\n","       smooth_loss=train_loss\n","    smooth_loss_epoch[epoch-1]=torch.tensor(smooth_loss)\n","\n","\n","    print(optimizer.param_groups[0]['lr'])\n","    print(smooth_loss_epoch[epoch-1])\n","    lr_epoch[epoch-1]=float(optimizer.param_groups[0]['lr'])\n","   \n","        \n","    results['train_loss'].append(train_loss)\n","    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, temp0)\n","    results['test_acc@1'].append(test_acc_1)\n","    results['test_acc@5'].append(test_acc_5)\n","    results['smooth_loss'].append(smooth_loss)\n","    results['lr_epoch'].append(optimizer.param_groups[0]['lr'])\n","    # save statistics\n","    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n","    data_frame.to_csv(csvfilename, index_label='epoch')\n","    if test_acc_1 > best_acc:\n","        best_acc = test_acc_1\n","        torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),'optimizer_state_dict':optimizer.state_dict(),'scheduler_state_dict':scheduler.state_dict(),'best_acc':best_acc}, modelfilename)\n","    test_acc_1_epoch[epoch-1]=test_acc_1\n","    test_acc_5_epoch[epoch-1]=test_acc_5\n","    \n","minloss_loc=torch.argmin(smooth_loss_epoch)\n","minloss_lr=lr_epoch[minloss_loc]\n","maxgradloss_loc=torch.argmin(torch.gradient(smooth_loss_epoch)[0])\n","maxgradloss_lr=lr_epoch[maxgradloss_loc]\n","initfactor=1/10\n","maxfactor=1/10\n","print(f'lr corresponding to minloss={minloss_lr}');\n","print(f'lr corresponding to max grad={maxgradloss_lr}');\n","print(f'suggested maxlr={minloss_lr*maxfactor}');\n","print(f'suggested minlr={max(minloss_lr*initfactor*maxfactor,maxgradloss_lr)}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kc2fmSbpO0Po","executionInfo":{"status":"aborted","timestamp":1652836464457,"user_tz":-480,"elapsed":40,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","df=pd.read_csv(csvfilename)\n","\n","f1=plt.figure()\n","plt.semilogx(df['lr_epoch'],df['smooth_loss'])  \n","plt.xlabel('learning rate')\n","plt.ylabel('smoothed trg epoch loss')\n","plt.show()\n","\n","f2=plt.figure()\n","plt.semilogx(df['lr_epoch'],df['test_acc@1'])\n","plt.title('Sim CLR with RAdam')\n","plt.xlabel('learning rate')\n","plt.ylabel('epoch Test Accuracy')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSvUi1B_tZod","executionInfo":{"status":"aborted","timestamp":1652836464458,"user_tz":-480,"elapsed":40,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"simclr_resnet_500_Adammultistep_v4.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"62ee5a27c50a43cf883d6306a0f85e00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8d60e8cdf1d4bafbebd0c1efb645f6c","IPY_MODEL_15299c55aa9f4ccfb2667b99318a786f","IPY_MODEL_c413769aa99f4593b5f4975b6ff7e83a"],"layout":"IPY_MODEL_5a7f2822e9484e8b9ceb1261300a294f"}},"c8d60e8cdf1d4bafbebd0c1efb645f6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8df16e45245f425c85346691eec10bb3","placeholder":"​","style":"IPY_MODEL_42b5e4a93be448bbb10023714d6af295","value":""}},"15299c55aa9f4ccfb2667b99318a786f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65460bce30fa4e4aa1e278f157895d6c","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66227592da44435ab0256fb9abd92a2a","value":170498071}},"c413769aa99f4593b5f4975b6ff7e83a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_037f87f320654c569df992dcb481a8db","placeholder":"​","style":"IPY_MODEL_f804fb57c7824e6d84078aeefea687db","value":" 170499072/? [00:11&lt;00:00, 16101556.50it/s]"}},"5a7f2822e9484e8b9ceb1261300a294f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8df16e45245f425c85346691eec10bb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42b5e4a93be448bbb10023714d6af295":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65460bce30fa4e4aa1e278f157895d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66227592da44435ab0256fb9abd92a2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"037f87f320654c569df992dcb481a8db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f804fb57c7824e6d84078aeefea687db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}