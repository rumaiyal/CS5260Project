{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HptO0CSwNjie"
      },
      "source": [
        "# Assignment 5 -- Contrastive Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkIokoYtNI_G"
      },
      "source": [
        "Please submit this file to Luminus by **23:59 on 20 Mar**. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. Finish 2 tasks according to the instructions. Only change the code in the required area and DO NOT change others or add new code/text snippets.\n",
        "2. Rename this file as \"Student_number.ipynb\". e.g., 'A0000000J.ipynb'. \n",
        "\n",
        "3. Submit the file to /Files/assignments/submission/assignment5. \n",
        "\n",
        "Please follow the instructions strictly, otherwise you might be penalized.\n",
        "\n",
        "If you has any questions, please propose it on Slack, or contact Ziheng Qin (e0823059@u.nus.edu) and Yong Liu (e0672130@u.nus.edu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXC8nEmxOMN6"
      },
      "source": [
        "First, we import the dataset and define transformation operations on it. We apply random transformation on images (crop + flip + colorjitter + grayscale)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hIWaDsVYG4OH"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "\n",
        "class CIFAR10Pair(CIFAR10):\n",
        "    \"\"\"CIFAR10 Dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            pos_1 = self.transform(img)\n",
        "            pos_2 = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return pos_1, pos_2, target\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct01fnfSNHuT"
      },
      "source": [
        "We use commonly used ResNet-50 as ConvNet encoders for simplicity in the original paper. The task 1 is to set encoder and projection head. The parameters are adapted from the original paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZbjYxzrgG6rO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.resnet import resnet50\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, feature_dim=128):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.f = []\n",
        "        for name, module in resnet50().named_children():\n",
        "            if name == 'conv1':\n",
        "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
        "                self.f.append(module)\n",
        "        # ----------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ----------------------------------------------------------------------\n",
        "        # Task 1\n",
        "        # set a neural network base encoder self.f\n",
        "        # hint: nn.Sequential\n",
        "        # Reference : https://github.com/leftthomas/SimCLR/blob/master/model.py\n",
        "        self.f = nn.Sequential(*self.f)\n",
        "\n",
        "\n",
        "        # set a small neural network projection head\n",
        "        # Dense-> Relu-> Dense (2-layer MLP to project the representation to a 128-dimensional latent space and \n",
        "        # the representation is 2048-dimensional here)\n",
        "        # Reference : https://github.com/leftthomas/SimCLR/blob/master/model.py\n",
        "        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512),\n",
        "                               nn.ReLU(inplace=True), nn.Linear(512, feature_dim, bias=True))\n",
        "     \n",
        "\n",
        "        \n",
        "        # ----------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ----------------------------------------------------------------------\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.g(feature)\n",
        "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPM5hsulQ74i"
      },
      "source": [
        "We train encoder network and projection head to maximize agreement using a contrastive loss. The default epoch is 1 for time efficiency while it could takes about 10 minutes to run for one epoch in google colab. The task 2 is to calculate the contrastive loss.\n",
        "To evaluate the influence of temperature value for contrastive loss, we run this training process 3 times with different temperature value (0.1,0.5 and 1.0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "92e253a6d1f248afad5e770f07eefa26",
            "a7919f0f341f479ba3f8eaacab613812",
            "ab9ab95c6b8b44e8a87ad7176740c0e0",
            "150a70a053214c59bb92070fbac0e11f",
            "7db7811bb64849e5a478a82f66b047e7",
            "20b41823f9de4dc7b6333228ce27c423",
            "ca9e5d93e33e4725b63b0acb637ac806",
            "0e7ee240bf714b7bbb98511771ddbdf0",
            "79fa3a5a8c9246ecbe3a5247a91d9640",
            "6c477d11e06a49678f26b3e8ae324a84",
            "46cd804aa3c347fb98f464e3447d6e3f"
          ]
        },
        "collapsed": true,
        "id": "w7FrLDw2HAWN",
        "outputId": "17a880f0-97e8-45a9-c538-145af4328aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (3.10.0.2)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.0.31.post2005241907\n",
            "Collecting torchlars\n",
            "  Downloading torchlars-0.1.2.tar.gz (6.5 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchlars) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchlars) (3.10.0.2)\n",
            "Building wheels for collected packages: torchlars\n",
            "  Building wheel for torchlars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchlars: filename=torchlars-0.1.2-cp37-cp37m-linux_x86_64.whl size=2199104 sha256=24704dd6bfa7324117102f41f40c9e9d57c146f8ec959cb4121c935abfa0b648\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/a9/96/62995a5abfd27e2f46671e17b99a52b6dccf46391d2b0e1c17\n",
            "Successfully built torchlars\n",
            "Installing collected packages: torchlars\n",
            "Successfully installed torchlars-0.1.2\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92e253a6d1f248afad5e770f07eefa26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Model'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/thop/vision/basic_hooks.py:92: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  kernel = torch.DoubleTensor([*(x[0].shape[2:])]) // torch.DoubleTensor(list((m.output_size,))).squeeze()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Model Params: 24.62M FLOPs: 1.31G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/390 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchlars/lars.py:140: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  p.grad.add_(weight_decay, p.data)\n",
            "Train Epoch: [1/501] Loss: 4.5991: 100%|██████████| 390/390 [00:57<00:00,  6.73it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.53it/s]\n",
            "Test Epoch: [1/501] Acc@1:43.32% Acc@5:90.62%: 100%|██████████| 79/79 [00:02<00:00, 28.26it/s]\n",
            "Train Epoch: [2/501] Loss: 3.6332: 100%|██████████| 390/390 [00:57<00:00,  6.80it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.48it/s]\n",
            "Test Epoch: [2/501] Acc@1:47.70% Acc@5:92.25%: 100%|██████████| 79/79 [00:02<00:00, 28.27it/s]\n",
            "Train Epoch: [3/501] Loss: 2.9951: 100%|██████████| 390/390 [00:57<00:00,  6.82it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.36it/s]\n",
            "Test Epoch: [3/501] Acc@1:50.28% Acc@5:93.25%: 100%|██████████| 79/79 [00:02<00:00, 28.64it/s]\n",
            "Train Epoch: [4/501] Loss: 2.5622: 100%|██████████| 390/390 [00:57<00:00,  6.80it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.51it/s]\n",
            "Test Epoch: [4/501] Acc@1:53.43% Acc@5:94.74%: 100%|██████████| 79/79 [00:02<00:00, 28.66it/s]\n",
            "Train Epoch: [5/501] Loss: 2.2601: 100%|██████████| 390/390 [00:57<00:00,  6.80it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.46it/s]\n",
            "Test Epoch: [5/501] Acc@1:56.71% Acc@5:95.52%: 100%|██████████| 79/79 [00:02<00:00, 28.73it/s]\n",
            "Train Epoch: [6/501] Loss: 2.0308: 100%|██████████| 390/390 [00:57<00:00,  6.80it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.54it/s]\n",
            "Test Epoch: [6/501] Acc@1:58.88% Acc@5:95.88%: 100%|██████████| 79/79 [00:02<00:00, 28.70it/s]\n",
            "Train Epoch: [7/501] Loss: 1.8781: 100%|██████████| 390/390 [00:57<00:00,  6.79it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.46it/s]\n",
            "Test Epoch: [7/501] Acc@1:59.76% Acc@5:96.28%: 100%|██████████| 79/79 [00:02<00:00, 28.38it/s]\n",
            "Train Epoch: [8/501] Loss: 1.7656: 100%|██████████| 390/390 [00:57<00:00,  6.81it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.55it/s]\n",
            "Test Epoch: [8/501] Acc@1:61.44% Acc@5:96.55%: 100%|██████████| 79/79 [00:02<00:00, 28.49it/s]\n",
            "Train Epoch: [9/501] Loss: 1.6736: 100%|██████████| 390/390 [00:57<00:00,  6.81it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.35it/s]\n",
            "Test Epoch: [9/501] Acc@1:62.50% Acc@5:96.74%: 100%|██████████| 79/79 [00:02<00:00, 28.51it/s]\n",
            "Train Epoch: [10/501] Loss: 1.5999: 100%|██████████| 390/390 [00:57<00:00,  6.80it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.41it/s]\n",
            "Test Epoch: [10/501] Acc@1:63.44% Acc@5:96.97%: 100%|██████████| 79/79 [00:02<00:00, 28.57it/s]\n",
            "Train Epoch: [11/501] Loss: 1.5435: 100%|██████████| 390/390 [00:57<00:00,  6.80it/s]\n",
            "Feature extracting: 100%|██████████| 391/391 [00:09<00:00, 42.51it/s]\n",
            "Test Epoch: [11/501] Acc@1:64.57% Acc@5:97.11%: 100%|██████████| 79/79 [00:02<00:00, 28.52it/s]\n",
            "Train Epoch: [12/501] Loss: 1.5402:  28%|██▊       | 110/390 [00:17<00:40,  6.99it/s]"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "#!pip install thop\n",
        "from thop import profile, clever_format\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "#!pip install torchlars\n",
        "from torchlars import LARS\n",
        "\n",
        "import pickle\n",
        "\n",
        "def contrastive_loss(out_1, out_2, temperature):\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    # Task2: implement contrastive loss function and return loss variable\n",
        "    # hint: loss formula could refer to the slides\n",
        "    # input: out_1, out_2，temperature\n",
        "    # output: loss variable\n",
        "\n",
        "    out = torch.cat([out_1, out_2], dim=0)\n",
        "    # [2*B, 2*B]\n",
        "    sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
        "    mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n",
        "    # [2*B, 2*B-1]\n",
        "    sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n",
        "\n",
        "    # compute loss\n",
        "    pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
        "    # [2*B]\n",
        "    pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
        "    loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean() \n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    return loss\n",
        "\n",
        "# train for one epoch to learn unique features\n",
        "def train(net, data_loader, train_optimizer, temperature):\n",
        "    net.train()\n",
        "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
        "    for pos_1, pos_2, target in train_bar:\n",
        "        pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)\n",
        "        feature_1, out_1 = net(pos_1)\n",
        "        feature_2, out_2 = net(pos_2)\n",
        "\n",
        "        loss = contrastive_loss(out_1, out_2, temperature)\n",
        "\n",
        "        train_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_optimizer.step()\n",
        "\n",
        "        total_num += batch_size\n",
        "        total_loss += loss.item() * batch_size\n",
        "        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n",
        "\n",
        "    return total_loss / total_num\n",
        "\n",
        "\n",
        "# test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n",
        "def test(net, memory_data_loader, test_data_loader, temperature):\n",
        "    net.eval()\n",
        "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "    with torch.no_grad():\n",
        "        # generate feature bank\n",
        "        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
        "            feature, out = net(data.cuda(non_blocking=True))\n",
        "            feature_bank.append(feature)\n",
        "        # [D, N]\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "        # [N]\n",
        "        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_data_loader)\n",
        "        for data, _, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            feature, out = net(data)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "            sim_matrix = torch.mm(feature, feature_bank)\n",
        "            # [B, K]\n",
        "            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n",
        "            # [B, K]\n",
        "            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n",
        "            sim_weight = (sim_weight / temperature).exp()\n",
        "\n",
        "            # counts for each class\n",
        "            one_hot_label = torch.zeros(data.size(0) * k, c, device=sim_labels.device)\n",
        "            # [B*K, C]\n",
        "            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
        "            # weighted score ---> [B, C]\n",
        "            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n",
        "                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n",
        "\n",
        "    return total_top1 / total_num * 100, total_top5 / total_num * 100\n",
        "\n",
        "# Train SimCLR\n",
        "   \n",
        "# Feature dim for latent vector, Temperature used in softmax, Top k most similar images used to predict the label\n",
        "feature_dim, temp, k = 128, [0.1, 0.5, 1], 200\n",
        "# Number of images in each mini-batch, Number of sweeps over the dataset to train\n",
        "# batch_size, epochs = 128, 501\n",
        "batch_size, epochs = 128, 501\n",
        "# data prepare\n",
        "train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True,\n",
        "                          drop_last=True)\n",
        "memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n",
        "memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "file_saveinfo=open('simclr_lars_500.txt','w')\n",
        "\n",
        "\n",
        "\n",
        "# training loop\n",
        "temp0=temp[0]\n",
        "\n",
        "# model setup and optimizer config\n",
        "model = Model(feature_dim).cuda()\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "base_optimizer=optim.Adam(model.parameters(),lr=1e-3/torch.pow(torch.tensor(2.0),0.5*((batch_size/128)-1)), weight_decay=1e-6)\n",
        "optimizer=LARS(optimizer=base_optimizer, eps=1e-8, trust_coef=0.001)\n",
        "c = len(memory_data.classes)\n",
        "\n",
        "results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "save_name_pre = '{}_{}_{}_{}_{}'.format(feature_dim, temp0, k, batch_size, epochs)\n",
        "if not os.path.exists('results'):\n",
        "    os.mkdir('results')\n",
        "best_acc = 0.0\n",
        "\n",
        "train_loss_epoch=torch.zeros(epochs)\n",
        "test_acc_1_epoch=torch.zeros(epochs)\n",
        "test_acc_5_epoch=torch.zeros(epochs)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(model, train_loader, optimizer, temp0)\n",
        "    train_loss_epoch[epoch-1]=train_loss\n",
        "        \n",
        "    results['train_loss'].append(train_loss)\n",
        "    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, temp0)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    results['test_acc@5'].append(test_acc_5)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "    data_frame.to_csv('results/{}_statistics.csv'.format(save_name_pre), index_label='epoch')\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        torch.save(model.state_dict(), 'results/{}_model.pth'.format(save_name_pre))\n",
        "    test_acc_1_epoch[epoch-1]=test_acc_1\n",
        "    test_acc_5_epoch[epoch-1]=test_acc_5\n",
        "    \n",
        "    file_saveinfo=open('simclr_lars_500.txt','wb')\n",
        "    pickle.dump([train_loss_epoch, test_acc_1_epoch, test_acc_5_epoch],file_saveinfo)\n",
        "    file_saveinfo.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(test_acc_1_epoch)"
      ],
      "metadata": {
        "id": "Kc2fmSbpO0Po"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "simclr_lars_500.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92e253a6d1f248afad5e770f07eefa26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7919f0f341f479ba3f8eaacab613812",
              "IPY_MODEL_ab9ab95c6b8b44e8a87ad7176740c0e0",
              "IPY_MODEL_150a70a053214c59bb92070fbac0e11f"
            ],
            "layout": "IPY_MODEL_7db7811bb64849e5a478a82f66b047e7"
          }
        },
        "a7919f0f341f479ba3f8eaacab613812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20b41823f9de4dc7b6333228ce27c423",
            "placeholder": "​",
            "style": "IPY_MODEL_ca9e5d93e33e4725b63b0acb637ac806",
            "value": ""
          }
        },
        "ab9ab95c6b8b44e8a87ad7176740c0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e7ee240bf714b7bbb98511771ddbdf0",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79fa3a5a8c9246ecbe3a5247a91d9640",
            "value": 170498071
          }
        },
        "150a70a053214c59bb92070fbac0e11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c477d11e06a49678f26b3e8ae324a84",
            "placeholder": "​",
            "style": "IPY_MODEL_46cd804aa3c347fb98f464e3447d6e3f",
            "value": " 170499072/? [00:02&lt;00:00, 89291163.53it/s]"
          }
        },
        "7db7811bb64849e5a478a82f66b047e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b41823f9de4dc7b6333228ce27c423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca9e5d93e33e4725b63b0acb637ac806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7ee240bf714b7bbb98511771ddbdf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fa3a5a8c9246ecbe3a5247a91d9640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c477d11e06a49678f26b3e8ae324a84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46cd804aa3c347fb98f464e3447d6e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}