{"cells":[{"cell_type":"markdown","metadata":{"id":"HptO0CSwNjie"},"source":["# Contrastive Learning on CIFAR10 using ConvNext Backbone"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIWaDsVYG4OH","outputId":"ba31a5bb-241f-4878-9d16-e128859eab42","executionInfo":{"status":"ok","timestamp":1652836649678,"user_tz":-480,"elapsed":20151,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"vXC8nEmxOMN6"},"source":["First, we import the dataset and define transformation operations on it. We apply random transformation on images (crop + flip + colorjitter + grayscale)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"c61bUj3dVpYD","executionInfo":{"status":"ok","timestamp":1652836652726,"user_tz":-480,"elapsed":3057,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","import torch\n","\n","class CIFAR10Pair(CIFAR10):\n","    \"\"\"CIFAR10 Dataset.\n","    \"\"\"\n","\n","    def __getitem__(self, index):\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            pos_1 = self.transform(img)\n","            pos_2 = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return pos_1, pos_2, target\n","\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(32),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n","    transforms.RandomGrayscale(p=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"]},{"cell_type":"markdown","metadata":{"id":"ct01fnfSNHuT"},"source":["We use commonly used ResNet-50 as ConvNet encoders for simplicity in the original paper. The task 1 is to set encoder and projection head. The parameters are adapted from the original paper."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZbjYxzrgG6rO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652836656133,"user_tz":-480,"elapsed":3411,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}},"outputId":"50308de9-3c28-4802-b481-7090a483c1af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}],"source":["# self written ViT code\n","# referenced from https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n","\n","!pip install einops\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.attend = nn.Softmax(dim = -1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n","\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","\n","        attn = self.attend(dots)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size=32, patch_size=8, num_classes=1000, dim=1024, depth=6, heads=16, mlp_dim=2048, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.1, emb_dropout = 0.1, feature_dim=128):\n","    #def __init__(self, *, image_size=256, patch_size=16, num_classes=1000, dim=1024, depth=6, heads=16, mlp_dim=2048, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.1, emb_dropout = 0.1, feature_dim=128):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.Linear(patch_dim, dim),\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","        #self.linear_crossover = nn.Linear(5120, mlp_dim)\n","        self.linear_crossover = nn.Linear(17408,mlp_dim) \n","\n","        self.g = nn.Sequential(nn.Linear(mlp_dim, 512, bias=False), \n","                               nn.BatchNorm1d(512),\n","                               nn.ReLU(inplace=True),\n","                               nn.Linear(512, feature_dim, bias=True))\n","\n","\n","        #comment out below\n","        #self.pool = pool\n","        #self.to_latent = nn.Identity()\n","        #self.mlp_head = nn.Sequential(\n","        #    nn.LayerNorm(dim),\n","        #    nn.Linear(dim, num_classes)\n","        #)\n","        \n","\n","\n","    def forward(self, x):\n","\n","        #print(\"x input shape: \", x.shape)\n","\n","        x = self.to_patch_embedding(x)\n","\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","\n","        x = self.dropout(x)\n","        #print(\"X after dropout layer: \", x.shape)\n","\n","        x = self.transformer(x)\n","        #print(\"X after transformer layer: \", x.shape)\n","\n","        x = torch.flatten(x,1)\n","        #print(\"X after reduce_dim layer: \", x.shape)\n","\n","        x = self.linear_crossover(x)\n","        #print(\"X after linear layer: \", x.shape)\n","\n","        feature = x\n","        #print(\"F after dimension removal: \", feature.shape)\n","\n","        out = self.g(feature)\n","        #out = feature\n","        #print(\"Out Shape: \", out.shape)\n","\n","\n","        #comment out below\n","        #x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","        #x = self.to_latent(x)\n","        #x = self.mlp_head(x)\n","\n","        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n","\n","        \n"]},{"cell_type":"markdown","metadata":{"id":"PPM5hsulQ74i"},"source":["We train encoder network and projection head to maximize agreement using a contrastive loss. The default epoch is 1 for time efficiency while it could takes about 10 minutes to run for one epoch in google colab. The task 2 is to calculate the contrastive loss.\n","To evaluate the influence of temperature value for contrastive loss, we run this training process 3 times with different temperature value (0.1,0.5 and 1.0)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7FrLDw2HAWN","outputId":"b119e51d-e76c-4f3f-935d-8659e4c2c8d0","executionInfo":{"status":"ok","timestamp":1652836659507,"user_tz":-480,"elapsed":3385,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thop\n","  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (4.2.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.0.31.post2005241907\n"]}],"source":["import argparse\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.optim as optim\n","!pip install thop\n","from thop import profile, clever_format\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","\n","import math\n","\n","def contrastive_loss(out_1, out_2, temperature):\n","\n","    # ------------------------------------------------------------------\n","    # START OF YOUR CODE\n","    # ------------------------------------------------------------------\n","    # Task2: implement contrastive loss function and return loss variable\n","    # hint: loss formula could refer to the slides\n","    # input: out_1, out_2，temperature\n","    # output: loss variable\n","\n","    out = torch.cat([out_1, out_2], dim=0)\n","    # [2*B, 2*B]\n","    sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n","    mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n","    # [2*B, 2*B-1]\n","    sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n","\n","    # compute loss\n","    pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n","    # [2*B]\n","    pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n","    loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean() \n","\n","    # ------------------------------------------------------------------\n","    # END OF YOUR CODE\n","    # ------------------------------------------------------------------\n","\n","    return loss\n","\n","# train for one epoch to learn unique features\n","def train(net, data_loader, train_optimizer, temperature):\n","    net.train()\n","    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n","    for pos_1, pos_2, target in train_bar:\n","        pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)\n","        feature_1, out_1 = net(pos_1)\n","        feature_2, out_2 = net(pos_2)\n","\n","        loss = contrastive_loss(out_1, out_2, temperature)\n","\n","        train_optimizer.zero_grad()\n","        loss.backward()\n","        train_optimizer.step()\n","        #train_scheduler.step()\n","\n","        total_num += batch_size\n","        total_loss += loss.item() * batch_size\n","        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n","\n","    return total_loss / total_num\n","\n","\n","# test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n","def test(net, memory_data_loader, test_data_loader, temperature):\n","    net.eval()\n","    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n","    with torch.no_grad():\n","        # generate feature bank\n","        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n","            feature, out = net(data.cuda(non_blocking=True))\n","            feature_bank.append(feature)\n","        # [D, N]\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        # [N]\n","        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n","        # loop test data to predict the label by weighted knn search\n","        test_bar = tqdm(test_data_loader)\n","        for data, _, target in test_bar:\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            feature, out = net(data)\n","\n","            total_num += data.size(0)\n","            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n","            sim_matrix = torch.mm(feature, feature_bank)\n","            # [B, K]\n","            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n","            # [B, K]\n","            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n","            sim_weight = (sim_weight / temperature).exp()\n","\n","            # counts for each class\n","            one_hot_label = torch.zeros(data.size(0) * k, c, device=sim_labels.device)\n","            # [B*K, C]\n","            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n","            # weighted score ---> [B, C]\n","            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n","\n","            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n","                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n","\n","    return total_top1 / total_num * 100, total_top5 / total_num * 100"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5nhS6egtxDk8","executionInfo":{"status":"ok","timestamp":1652836659508,"user_tz":-480,"elapsed":14,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d6e9c719cb854449a56184511b5f65a5","4a9c4a33f95f4608995ce76d80290d6f","61e1cc8e94054549a5bdd9f685a3fc60","aa515d4cc9f5459aade40e08557f7ce1","6cbe7117ea2d418ca593826a83ab2d72","572b08dd787540ecb23338a5949b8545","3793a7f243cf43d1b175bea21304e15d","386b9e06fd1846f8ba662c2c8b92bbe9","879555e650dd44569b2091fb95725b07","42617efbcc384df996725b1382bd8232","d2fc57bdc5254a07b549c4264b7b74a9"]},"id":"BCIhOUGGxT93","outputId":"48b65b27-e13f-4fa3-dba4-143b8fa0413f","scrolled":true,"executionInfo":{"status":"error","timestamp":1652837133958,"user_tz":-480,"elapsed":474461,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e9c719cb854449a56184511b5f65a5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","\u001b[91m[WARN] Cannot find rule for <class 'einops.layers.torch.Rearrange'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.normalization.LayerNorm'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.PreNorm'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.GELU'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.FeedForward'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.ModuleList'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.Transformer'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.ViT'>. Treat it as zero Macs and zero Params.\u001b[00m\n","# Model Params: 87.32M FLOPs: 895.55M\n","[5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330]\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: [1/500] Loss: 4.0251: 100%|██████████| 390/390 [01:33<00:00,  4.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.00015\n","tensor(4.0251)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:19<00:00, 19.64it/s]\n","Test Epoch: [1/500] Acc@1:47.73% Acc@5:91.67%: 100%|██████████| 79/79 [00:05<00:00, 14.29it/s]\n","Train Epoch: [2/500] Loss: 3.3255: 100%|██████████| 390/390 [01:37<00:00,  4.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.00015\n","tensor(3.3605)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:20<00:00, 19.39it/s]\n","Test Epoch: [2/500] Acc@1:48.63% Acc@5:92.44%: 100%|██████████| 79/79 [00:05<00:00, 13.91it/s]\n","Train Epoch: [3/500] Loss: 3.0045: 100%|██████████| 390/390 [01:37<00:00,  4.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["0.00015\n","tensor(3.0223)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:20<00:00, 19.38it/s]\n","Test Epoch: [3/500] Acc@1:49.63% Acc@5:92.75%: 100%|██████████| 79/79 [00:05<00:00, 14.07it/s]\n","Train Epoch: [4/500] Loss: 2.8535:  60%|██████    | 235/390 [01:06<00:43,  3.55it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-498138592820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mtrain_loss_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-4b8f40a5278a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, train_optimizer, temperature)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mpos_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mfeature_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mfeature_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-43b9a0c2bb9c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m#print(\"F after dimension removal: \", feature.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;31m#out = feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m#print(\"Out Shape: \", out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train SimCLR\n","import numpy as np\n","   \n","# Feature dim for latent vector, Temperature used in softmax, Top k most similar images used to predict the label\n","feature_dim, temp, k = 128, [0.1], 200\n","# Number of images in each mini-batch, Number of sweeps over the dataset to train\n","batch_size=128\n","max_lrvalue=1.5e-4 #Rule of Thumb is peaklr (from lambdalr test)*3/8\n","min_lrvalue=9.0e-6\n","epochs=500\n","numsteps=np.max([epochs*0.67/5,10]).astype(int) \n","temp0 = 0.1 #contrastive loss temperature setting\n","schedulertype='multisteplr' #'multisteplr' for actual training\n","if schedulertype=='multisteplr':\n","    str1=str(min_lrvalue)\n","    str2=str(max_lrvalue)\n","    str3=str(numsteps)\n","    #epochs=3\n","else:\n","    print('choose valid option for scheduler')\n","smoothfactor=0.95 #Smooth Factor for smoothing contrastive loss    \n","IterationStr='It1'\n","loadmodel=0 #loadmodel=0 From scratch or loadmodel=1 Continue from presaved model \n","pathtosavemodel='/content/gdrive/MyDrive/CS5260Project/results/Simclr_vit_Adamv4'+schedulertype+'_minlr_'+str1+'_maxlr_'+str2+'_numsteps_'+str3+'/'+IterationStr+'/model/'\n","pathtosavecsv='/content/gdrive/MyDrive/CS5260Project/results/Simclr_vit_Adamv4'+schedulertype+'_minlr_'+str1+'_maxlr_'+str2+'_numsteps_'+str3+'/'+IterationStr+'/csv/'\n","save_name_pre = '{}_{}_{}_{}_{}'.format(feature_dim, temp0, k, batch_size, epochs)\n","csvfilename=pathtosavecsv+'{}_statistics.csv'.format(save_name_pre)\n","modelfilename=pathtosavemodel+'{}_model.pth'.format(save_name_pre)\n","\n","# data prepare\n","train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True,\n","                          drop_last=True)\n","memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n","memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n","test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n","\n","import torch\n","torch.cuda.is_available()\n","n=1\n","#torch.cuda.set_device(n)\n","\n","# model setup and optimizer config\n","model = ViT().cuda()\n","\n","flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n","flops, params = clever_format([flops, params])\n","print('# Model Params: {} FLOPs: {}'.format(params, flops))\n","\n","optimizer = optim.Adam(model.parameters(), weight_decay=1e-6, lr=max_lrvalue)\n","if schedulertype=='multisteplr':\n","   gammaval=2**(math.log2(min_lrvalue/max_lrvalue)/numsteps)\n","   milestones_list=np.rint(np.arange(0,numsteps)/numsteps*epochs*0.67)[1:].astype(int).tolist() \n","   print(milestones_list)\n","   scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=milestones_list,gamma=gammaval)\n","else:\n","   print('choose valid option for scheduler')\n","\n","if loadmodel==1:\n","   checkpoint=torch.load(modelfilename)\n","   model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n","   model.to(device)\n","   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","   scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","   startepoch=checkpoint['epoch']+1\n","   best_acc=checkpoint['best_acc']\n","\n","   print(startepoch)\n","else:     \n","   startepoch=1\n","   best_acc=0\n","   \n","\n","c = len(memory_data.classes)\n","\n","if not os.path.exists(pathtosavemodel):\n","   os.makedirs(pathtosavemodel)\n","if not os.path.exists(pathtosavecsv):\n","   os.makedirs(pathtosavecsv)\n","\n","train_loss_epoch=torch.zeros(epochs)\n","smooth_loss_epoch=torch.zeros(epochs)\n","test_acc_1_epoch=torch.zeros(epochs)\n","test_acc_5_epoch=torch.zeros(epochs)\n","lr_epoch=torch.zeros(epochs)\n","\n","if loadmodel==1:\n","\n","   df=pd.read_csv(csvfilename)\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,1]).apply(np.array)\n","   train_loss_epoch[0:temp.size]=torch.tensor(temp)\n","   train_loss_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,2]).apply(np.array)\n","   test_acc_1_epoch[0:temp.size]=torch.tensor(temp)\n","   test_acc_1_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,3]).apply(np.array)\n","   test_acc_5_epoch[0:temp.size]=torch.tensor(temp)\n","   test_acc_5_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,4]).apply(np.array)\n","   smooth_loss_epoch[0:temp.size]=torch.tensor(temp)\n","   smooth_loss_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,5]).apply(np.array)\n","   lr_epoch[0:temp.size]=torch.tensor(temp)\n","   lr_list=temp.tolist()\n","   results = {'train_loss': train_loss_list, 'test_acc@1': test_acc_1_list, 'test_acc@5': test_acc_5_list, 'smooth_loss': smooth_loss_list, 'lr_epoch': lr_list}\n","\n","else:\n","   results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': [], 'smooth_loss': [], 'lr_epoch': []}\n","\n","\n","for epoch in range(startepoch, epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, temp0)\n","    scheduler.step()\n","    train_loss_epoch[epoch-1]=train_loss\n","    if epoch>1:\n","       smooth_loss=float(train_loss_epoch[epoch-1]*smoothfactor+smooth_loss_epoch[epoch-2]*(1.0-smoothfactor))\n","    else:\n","       smooth_loss=train_loss\n","    smooth_loss_epoch[epoch-1]=torch.tensor(smooth_loss)\n","\n","\n","    print(optimizer.param_groups[0]['lr'])\n","    print(smooth_loss_epoch[epoch-1])\n","    lr_epoch[epoch-1]=float(optimizer.param_groups[0]['lr'])\n","   \n","        \n","    results['train_loss'].append(train_loss)\n","    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, temp0)\n","    results['test_acc@1'].append(test_acc_1)\n","    results['test_acc@5'].append(test_acc_5)\n","    results['smooth_loss'].append(smooth_loss)\n","    results['lr_epoch'].append(optimizer.param_groups[0]['lr'])\n","    # save statistics\n","    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n","    data_frame.to_csv(csvfilename, index_label='epoch')\n","    if test_acc_1 > best_acc:\n","        best_acc = test_acc_1\n","        torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),'optimizer_state_dict':optimizer.state_dict(),'scheduler_state_dict':scheduler.state_dict(),'best_acc':best_acc}, modelfilename)\n","    test_acc_1_epoch[epoch-1]=test_acc_1\n","    test_acc_5_epoch[epoch-1]=test_acc_5\n","    \n","minloss_loc=torch.argmin(smooth_loss_epoch)\n","minloss_loclr=lr_epoch[minloss_loc]\n","print(f'lr corresponding to minloss={minloss_loclr}');\n","print(f'suggested maxlr={minloss_loclr*3/8}');\n","print(f'suggested minlr={minloss_loclr*3/80}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kc2fmSbpO0Po","executionInfo":{"status":"aborted","timestamp":1652837133956,"user_tz":-480,"elapsed":31,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","df=pd.read_csv(csvfilename)\n","\n","f1=plt.figure()\n","plt.semilogx(df['lr_epoch'],df['smooth_loss'])  \n","plt.xlabel('learning rate')\n","plt.ylabel('smoothed trg epoch loss')\n","plt.show()\n","\n","f2=plt.figure()\n","plt.plot(df['test_acc@1'])\n","plt.title('Sim CLR with RAdam')\n","plt.xlabel('epoch number')\n","plt.ylabel('epoch Test Accuracy')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"simclr_vit_500_Adammultistep_v4.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d6e9c719cb854449a56184511b5f65a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a9c4a33f95f4608995ce76d80290d6f","IPY_MODEL_61e1cc8e94054549a5bdd9f685a3fc60","IPY_MODEL_aa515d4cc9f5459aade40e08557f7ce1"],"layout":"IPY_MODEL_6cbe7117ea2d418ca593826a83ab2d72"}},"4a9c4a33f95f4608995ce76d80290d6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_572b08dd787540ecb23338a5949b8545","placeholder":"​","style":"IPY_MODEL_3793a7f243cf43d1b175bea21304e15d","value":""}},"61e1cc8e94054549a5bdd9f685a3fc60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_386b9e06fd1846f8ba662c2c8b92bbe9","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_879555e650dd44569b2091fb95725b07","value":170498071}},"aa515d4cc9f5459aade40e08557f7ce1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42617efbcc384df996725b1382bd8232","placeholder":"​","style":"IPY_MODEL_d2fc57bdc5254a07b549c4264b7b74a9","value":" 170499072/? [00:05&lt;00:00, 31056596.61it/s]"}},"6cbe7117ea2d418ca593826a83ab2d72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"572b08dd787540ecb23338a5949b8545":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3793a7f243cf43d1b175bea21304e15d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"386b9e06fd1846f8ba662c2c8b92bbe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"879555e650dd44569b2091fb95725b07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42617efbcc384df996725b1382bd8232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2fc57bdc5254a07b549c4264b7b74a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}