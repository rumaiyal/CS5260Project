{"cells":[{"cell_type":"markdown","metadata":{"id":"HptO0CSwNjie"},"source":["# Contrastive Learning on CIFAR10 using ViT Backbone"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIWaDsVYG4OH","outputId":"70fbea31-e0bd-4ae1-fcbf-39276296371c","executionInfo":{"status":"ok","timestamp":1652742514738,"user_tz":-480,"elapsed":20274,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"vXC8nEmxOMN6"},"source":["First, we import the dataset and define transformation operations on it. We apply random transformation on images (crop + flip + colorjitter + grayscale)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"c61bUj3dVpYD","executionInfo":{"status":"ok","timestamp":1652742517215,"user_tz":-480,"elapsed":2484,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","import torch\n","\n","class CIFAR10Pair(CIFAR10):\n","    \"\"\"CIFAR10 Dataset.\n","    \"\"\"\n","\n","    def __getitem__(self, index):\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            pos_1 = self.transform(img)\n","            pos_2 = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return pos_1, pos_2, target\n","\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(32),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n","    transforms.RandomGrayscale(p=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])"]},{"cell_type":"markdown","metadata":{"id":"ct01fnfSNHuT"},"source":["We use commonly used ResNet-50 as ConvNet encoders for simplicity in the original paper. The task 1 is to set encoder and projection head. The parameters are adapted from the original paper."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZbjYxzrgG6rO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652742520558,"user_tz":-480,"elapsed":3347,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}},"outputId":"a92dc9fa-a9c5-4765-e720-c4d2adadf3e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}],"source":["# self written ViT code\n","# referenced from https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n","\n","!pip install einops\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        project_out = not (heads == 1 and dim_head == dim)\n","\n","        self.heads = heads\n","        self.scale = dim_head ** -0.5\n","\n","        self.attend = nn.Softmax(dim = -1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        ) if project_out else nn.Identity()\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n","\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","\n","        attn = self.attend(dots)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size=32, patch_size=8, num_classes=1000, dim=1024, depth=6, heads=16, mlp_dim=2048, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.1, emb_dropout = 0.1, feature_dim=128):\n","    #def __init__(self, *, image_size=256, patch_size=16, num_classes=1000, dim=1024, depth=6, heads=16, mlp_dim=2048, pool = 'cls', channels = 3, dim_head = 64, dropout = 0.1, emb_dropout = 0.1, feature_dim=128):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n","            nn.Linear(patch_dim, dim),\n","        )\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","        #self.linear_crossover = nn.Linear(5120, mlp_dim)\n","        self.linear_crossover = nn.Linear(17408,mlp_dim) \n","\n","        self.g = nn.Sequential(nn.Linear(mlp_dim, 512, bias=False), \n","                               nn.BatchNorm1d(512),\n","                               nn.ReLU(inplace=True),\n","                               nn.Linear(512, feature_dim, bias=True))\n","\n","\n","        #comment out below\n","        #self.pool = pool\n","        #self.to_latent = nn.Identity()\n","        #self.mlp_head = nn.Sequential(\n","        #    nn.LayerNorm(dim),\n","        #    nn.Linear(dim, num_classes)\n","        #)\n","        \n","\n","\n","    def forward(self, x):\n","\n","        #print(\"x input shape: \", x.shape)\n","\n","        x = self.to_patch_embedding(x)\n","\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","\n","        x = self.dropout(x)\n","        #print(\"X after dropout layer: \", x.shape)\n","\n","        x = self.transformer(x)\n","        #print(\"X after transformer layer: \", x.shape)\n","\n","        x = torch.flatten(x,1)\n","        #print(\"X after reduce_dim layer: \", x.shape)\n","\n","        x = self.linear_crossover(x)\n","        #print(\"X after linear layer: \", x.shape)\n","\n","        feature = x\n","        #print(\"F after dimension removal: \", feature.shape)\n","\n","        out = self.g(feature)\n","        #out = feature\n","        #print(\"Out Shape: \", out.shape)\n","\n","\n","        #comment out below\n","        #x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","        #x = self.to_latent(x)\n","        #x = self.mlp_head(x)\n","\n","        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n","\n","        \n"]},{"cell_type":"markdown","metadata":{"id":"PPM5hsulQ74i"},"source":["We train encoder network and projection head to maximize agreement using a contrastive loss. The default epoch is 1 for time efficiency while it could takes about 10 minutes to run for one epoch in google colab. The task 2 is to calculate the contrastive loss.\n","To evaluate the influence of temperature value for contrastive loss, we run this training process 3 times with different temperature value (0.1,0.5 and 1.0)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7FrLDw2HAWN","outputId":"684781d5-64e1-4024-c1dc-90d3bb997ed4","executionInfo":{"status":"ok","timestamp":1652742524334,"user_tz":-480,"elapsed":3371,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thop\n","  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from thop) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->thop) (4.2.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.0.31.post2005241907\n"]}],"source":["import argparse\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.optim as optim\n","!pip install thop\n","from thop import profile, clever_format\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","\n","import math\n","\n","def contrastive_loss(out_1, out_2, temperature):\n","\n","    # ------------------------------------------------------------------\n","    # START OF YOUR CODE\n","    # ------------------------------------------------------------------\n","    # Task2: implement contrastive loss function and return loss variable\n","    # hint: loss formula could refer to the slides\n","    # input: out_1, out_2，temperature\n","    # output: loss variable\n","\n","    out = torch.cat([out_1, out_2], dim=0)\n","    # [2*B, 2*B]\n","    sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n","    mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n","    # [2*B, 2*B-1]\n","    sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n","\n","    # compute loss\n","    pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n","    # [2*B]\n","    pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n","    loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean() \n","\n","    # ------------------------------------------------------------------\n","    # END OF YOUR CODE\n","    # ------------------------------------------------------------------\n","\n","    return loss\n","\n","# train for one epoch to learn unique features\n","def train(net, data_loader, train_optimizer, train_scheduler, temperature):\n","    net.train()\n","    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n","    for pos_1, pos_2, target in train_bar:\n","        pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)\n","        feature_1, out_1 = net(pos_1)\n","        feature_2, out_2 = net(pos_2)\n","\n","        loss = contrastive_loss(out_1, out_2, temperature)\n","\n","        train_optimizer.zero_grad()\n","        loss.backward()\n","        train_optimizer.step()\n","        train_scheduler.step()\n","\n","        total_num += batch_size\n","        total_loss += loss.item() * batch_size\n","        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n","\n","    return total_loss / total_num\n","\n","\n","# test for one epoch, use weighted knn to find the most similar images' label to assign the test image\n","def test(net, memory_data_loader, test_data_loader, temperature):\n","    net.eval()\n","    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n","    with torch.no_grad():\n","        # generate feature bank\n","        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):\n","            feature, out = net(data.cuda(non_blocking=True))\n","            feature_bank.append(feature)\n","        # [D, N]\n","        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n","        # [N]\n","        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n","        # loop test data to predict the label by weighted knn search\n","        test_bar = tqdm(test_data_loader)\n","        for data, _, target in test_bar:\n","            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n","            feature, out = net(data)\n","\n","            total_num += data.size(0)\n","            # compute cos similarity between each feature vector and feature bank ---> [B, N]\n","            sim_matrix = torch.mm(feature, feature_bank)\n","            # [B, K]\n","            sim_weight, sim_indices = sim_matrix.topk(k=k, dim=-1)\n","            # [B, K]\n","            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)\n","            sim_weight = (sim_weight / temperature).exp()\n","\n","            # counts for each class\n","            one_hot_label = torch.zeros(data.size(0) * k, c, device=sim_labels.device)\n","            # [B*K, C]\n","            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n","            # weighted score ---> [B, C]\n","            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, c) * sim_weight.unsqueeze(dim=-1), dim=1)\n","\n","            pred_labels = pred_scores.argsort(dim=-1, descending=True)\n","            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n","            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'\n","                                     .format(epoch, epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))\n","\n","    return total_top1 / total_num * 100, total_top5 / total_num * 100"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5nhS6egtxDk8","executionInfo":{"status":"ok","timestamp":1652742524336,"user_tz":-480,"elapsed":17,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3c6982a397794d7393335631b9bab347","335ecaee95d34d909e128eb76a6bb245","1cab2e8fd8d545a4a8eb67df6590c494","9cca9575768a49baa75b264d7330ed28","22782bb7acc241a493ecdbfcc9cc5e37","3dde1e6f014949a08b4ffddf15cea92b","fe1999bf5f0d4cdb9344cc02e9d47d4c","83db42bc5d6f4437af5e4294c843370b","c330afa2a79c48fabd46230a8c9f301b","f444f9f131f043319f259cc43f8a7e82","0231221238534ba5a1a52224eaa2e76f"]},"id":"BCIhOUGGxT93","outputId":"6f56bb96-d4d4-4373-d31f-d68a161d2292","executionInfo":{"status":"error","timestamp":1652743091805,"user_tz":-480,"elapsed":567483,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c6982a397794d7393335631b9bab347"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","\u001b[91m[WARN] Cannot find rule for <class 'einops.layers.torch.Rearrange'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.normalization.LayerNorm'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.Attention'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.PreNorm'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.GELU'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.FeedForward'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.ModuleList'>. Treat it as zero Macs and zero Params.\u001b[00m\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.Transformer'>. Treat it as zero Macs and zero Params.\u001b[00m\n","[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n","\u001b[91m[WARN] Cannot find rule for <class '__main__.ViT'>. Treat it as zero Macs and zero Params.\u001b[00m\n","# Model Params: 87.32M FLOPs: 895.55M\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch: [1/500] Loss: 4.4136: 100%|██████████| 390/390 [01:31<00:00,  4.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["7.007457016784506e-05\n","tensor(4.4136)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:19<00:00, 20.27it/s]\n","Test Epoch: [1/500] Acc@1:40.30% Acc@5:88.22%: 100%|██████████| 79/79 [00:05<00:00, 14.43it/s]\n","Train Epoch: [2/500] Loss: 3.9409: 100%|██████████| 390/390 [01:35<00:00,  4.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["7.029824796132161e-05\n","tensor(3.9646)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:19<00:00, 19.84it/s]\n","Test Epoch: [2/500] Acc@1:44.09% Acc@5:90.42%: 100%|██████████| 79/79 [00:05<00:00, 14.18it/s]\n","Train Epoch: [3/500] Loss: 3.5232: 100%|██████████| 390/390 [01:37<00:00,  4.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["7.067093526460263e-05\n","tensor(3.5453)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:19<00:00, 19.64it/s]\n","Test Epoch: [3/500] Acc@1:48.45% Acc@5:92.05%: 100%|██████████| 79/79 [00:05<00:00, 13.77it/s]\n","Train Epoch: [4/500] Loss: 3.1645: 100%|██████████| 390/390 [01:34<00:00,  4.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["7.119246859913083e-05\n","tensor(3.1836)\n"]},{"output_type":"stream","name":"stderr","text":["Feature extracting: 100%|██████████| 391/391 [00:19<00:00, 19.74it/s]\n","Test Epoch: [4/500] Acc@1:50.00% Acc@5:92.66%: 100%|██████████| 79/79 [00:05<00:00, 14.36it/s]\n","Train Epoch: [5/500] Loss: 3.0013:  37%|███▋      | 146/390 [00:42<01:10,  3.45it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0b54facda2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mtrain_loss_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-14ca1005bd68>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, train_optimizer, train_scheduler, temperature)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train SimCLR\n","import numpy as np\n","   \n","# Feature dim for latent vector, Temperature used in softmax, Top k most similar images used to predict the label\n","feature_dim, temp, k = 128, [0.1], 200\n","# Number of images in each mini-batch, Number of sweeps over the dataset to train\n","batch_size=128\n","#Coarse setting\n","#Fine setting\n","lr_start = 1.0e-6\n","lr_end =100\n","max_lrvalue=0.00075 #Rule of Thumb is peaklr (from lambdalr test)*3/8\n","min_lrvalue=7.0e-5\n","temp0 = 0.1 #contrastive loss temperature setting\n","schedulertype='onecyclelr' #'lambdalr' for testing range of training or 'onecyclelr' for actual training\n","if schedulertype=='lambdalr':#\n","    epochs=10\n","    str1=str(lr_start)\n","    str2=str(lr_end)\n","    str3=str('_')\n","    #epochs=10 #coarserg recommend 10 epochs, finerg recommend 100 epochs \n","elif schedulertype=='onecyclelr':\n","    div_factorvalue=max_lrvalue/min_lrvalue\n","    finaldiv_factorvalue=10000\n","    epochs=500\n","    str1=str(max_lrvalue/div_factorvalue)\n","    str2=str(max_lrvalue)\n","    str3=str(max_lrvalue/div_factorvalue/finaldiv_factorvalue)\n","    #epochs=3\n","else:\n","    print('choose valid option for scheduler')\n","smoothfactor=0.95 #Smooth Factor for smoothing contrastive loss    \n","IterationStr='It1'\n","loadmodel=0 #loadmodel=0 From scratch or loadmodel=1 Continue from presaved model \n","pathtosave='/content/gdrive/MyDrive/CS5260Project/results/Simclr_vit_RAdamv4'+schedulertype+'_minlr_'+str1+'_maxlr_'+str2+'_finallr_'+str3+'/'+IterationStr+'/'\n","save_name_pre = '{}_{}_{}_{}_{}'.format(feature_dim, temp0, k, batch_size, epochs)\n","csvfilename=pathtosave+'{}_statistics.csv'.format(save_name_pre)\n","modelfilename=pathtosave+'{}_model.pth'.format(save_name_pre)\n","\n","# data prepare\n","train_data = CIFAR10Pair(root='data', train=True, transform=train_transform, download=True)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True,\n","                          drop_last=True)\n","memory_data = CIFAR10Pair(root='data', train=True, transform=test_transform, download=True)\n","memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n","test_data = CIFAR10Pair(root='data', train=False, transform=test_transform, download=True)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n","\n","import torch\n","torch.cuda.is_available()\n","n=1\n","#torch.cuda.set_device(n)\n","\n","# model setup and optimizer config\n","model = ViT().cuda()\n","\n","flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n","flops, params = clever_format([flops, params])\n","print('# Model Params: {} FLOPs: {}'.format(params, flops))\n","\n","optimizer = optim.RAdam(model.parameters(), lr=1.0)\n","#optimizer=optim.Adam(model.parameters(),lr=1.0)\n","\n","#exponentially increase learning rate from low to high\n","def lrs(batch):\n","   low = math.log2(lr_start)\n","   high = math.log2(lr_end)\n","   return 2**(low+(high-low)*batch/len(train_loader)/epochs)\n","   \n","if schedulertype=='lambdalr':\n","   scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lrs)\n","elif schedulertype=='onecyclelr':\n","   scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,div_factor=div_factorvalue,final_div_factor=finaldiv_factorvalue,max_lr=max_lrvalue,total_steps=len(train_loader)*epochs,epochs=epochs)\n","else:\n","   print('choose valid option for scheduler')\n","\n","if loadmodel==1:\n","   checkpoint=torch.load(modelfilename)\n","   model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n","   model.to(device)\n","   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","   scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","   startepoch=checkpoint['epoch']+1\n","   best_acc=checkpoint['best_acc']\n","\n","   print(startepoch)\n","else:     \n","   startepoch=1\n","   best_acc=0\n","   \n","\n","c = len(memory_data.classes)\n","\n","if not os.path.exists(pathtosave):\n","   os.makedirs(pathtosave)\n","\n","train_loss_epoch=torch.zeros(epochs)\n","smooth_loss_epoch=torch.zeros(epochs)\n","test_acc_1_epoch=torch.zeros(epochs)\n","test_acc_5_epoch=torch.zeros(epochs)\n","lr_epoch=torch.zeros(epochs)\n","\n","if loadmodel==1:\n","\n","   df=pd.read_csv(csvfilename)\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,1]).apply(np.array)\n","   train_loss_epoch[0:temp.size]=torch.tensor(temp)\n","   train_loss_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,2]).apply(np.array)\n","   test_acc_1_epoch[0:temp.size]=torch.tensor(temp)\n","   test_acc_1_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,3]).apply(np.array)\n","   test_acc_5_epoch[0:temp.size]=torch.tensor(temp)\n","   test_acc_5_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,4]).apply(np.array)\n","   smooth_loss_epoch[0:temp.size]=torch.tensor(temp)\n","   smooth_loss_list=temp.tolist()\n","   temp=pd.to_numeric(df.iloc[0:startepoch-1,5]).apply(np.array)\n","   lr_epoch[0:temp.size]=torch.tensor(temp)\n","   lr_list=temp.tolist()\n","   results = {'train_loss': train_loss_list, 'test_acc@1': test_acc_1_list, 'test_acc@5': test_acc_5_list, 'smooth_loss': smooth_loss_list, 'lr_epoch': lr_list}\n","\n","else:\n","   results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': [], 'smooth_loss': [], 'lr_epoch': []}\n","\n","\n","for epoch in range(startepoch, epochs + 1):\n","    train_loss = train(model, train_loader, optimizer, scheduler, temp0)\n","    train_loss_epoch[epoch-1]=train_loss\n","    if epoch>1:\n","       smooth_loss=float(train_loss_epoch[epoch-1]*smoothfactor+smooth_loss_epoch[epoch-2]*(1.0-smoothfactor))\n","    else:\n","       smooth_loss=train_loss\n","    smooth_loss_epoch[epoch-1]=torch.tensor(smooth_loss)\n","\n","\n","    print(optimizer.param_groups[0]['lr'])\n","    print(smooth_loss_epoch[epoch-1])\n","    lr_epoch[epoch-1]=float(optimizer.param_groups[0]['lr'])\n","   \n","        \n","    results['train_loss'].append(train_loss)\n","    test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, temp0)\n","    results['test_acc@1'].append(test_acc_1)\n","    results['test_acc@5'].append(test_acc_5)\n","    results['smooth_loss'].append(smooth_loss)\n","    results['lr_epoch'].append(optimizer.param_groups[0]['lr'])\n","    # save statistics\n","    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n","    data_frame.to_csv(csvfilename, index_label='epoch')\n","    if test_acc_1 > best_acc:\n","        best_acc = test_acc_1\n","        torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),'optimizer_state_dict':optimizer.state_dict(),'scheduler_state_dict':scheduler.state_dict(),'best_acc':best_acc}, modelfilename)\n","    test_acc_1_epoch[epoch-1]=test_acc_1\n","    test_acc_5_epoch[epoch-1]=test_acc_5\n","    \n","minloss_loc=torch.argmin(smooth_loss_epoch)\n","minloss_loclr=lr_epoch[minloss_loc]\n","print(f'lr corresponding to minloss={minloss_loclr}');\n","print(f'suggested maxlr={minloss_loclr*3/8}');\n","print(f'suggested minlr={minloss_loclr*3/80}');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kc2fmSbpO0Po","executionInfo":{"status":"aborted","timestamp":1652743091804,"user_tz":-480,"elapsed":36,"user":{"displayName":"Umaiyal Ramanathan","userId":"12104065804675727870"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","df=pd.read_csv(csvfilename)\n","\n","f1=plt.figure()\n","plt.semilogx(df['lr_epoch'],df['smooth_loss'])  \n","plt.xlabel('learning rate')\n","plt.ylabel('smoothed trg epoch loss')\n","plt.show()\n","\n","f2=plt.figure()\n","plt.plot(df['test_acc@1'])\n","plt.title('Sim CLR with RAdam')\n","plt.xlabel('epoch number')\n","plt.ylabel('epoch Test Accuracy')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"simclr_vit_500_RAdamonecycle_v4.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3c6982a397794d7393335631b9bab347":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_335ecaee95d34d909e128eb76a6bb245","IPY_MODEL_1cab2e8fd8d545a4a8eb67df6590c494","IPY_MODEL_9cca9575768a49baa75b264d7330ed28"],"layout":"IPY_MODEL_22782bb7acc241a493ecdbfcc9cc5e37"}},"335ecaee95d34d909e128eb76a6bb245":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dde1e6f014949a08b4ffddf15cea92b","placeholder":"​","style":"IPY_MODEL_fe1999bf5f0d4cdb9344cc02e9d47d4c","value":""}},"1cab2e8fd8d545a4a8eb67df6590c494":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83db42bc5d6f4437af5e4294c843370b","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c330afa2a79c48fabd46230a8c9f301b","value":170498071}},"9cca9575768a49baa75b264d7330ed28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f444f9f131f043319f259cc43f8a7e82","placeholder":"​","style":"IPY_MODEL_0231221238534ba5a1a52224eaa2e76f","value":" 170499072/? [00:06&lt;00:00, 29414012.52it/s]"}},"22782bb7acc241a493ecdbfcc9cc5e37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dde1e6f014949a08b4ffddf15cea92b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe1999bf5f0d4cdb9344cc02e9d47d4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83db42bc5d6f4437af5e4294c843370b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c330afa2a79c48fabd46230a8c9f301b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f444f9f131f043319f259cc43f8a7e82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0231221238534ba5a1a52224eaa2e76f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}